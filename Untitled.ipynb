{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparams import Hyperparams as hp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "vocab = u'''␀␃ !',-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'''\n",
    "vocab_size=len(vocab)\n",
    "embed_size=256\n",
    "har_to_ix = {char: i for i, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input text to character embedding\n",
    "    input : A tensor with 1 or more dimension ((batch size), Text length)\n",
    "    already indexed at the data_load()\n",
    "    \n",
    "    num_units : length of the text input which is padded (# of hidden units) - 188\n",
    "    \n",
    "    embed_size : embed 1 character to 256 embed size - 256\n",
    "\n",
    "    output : (N, Tx, E)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_units, zero_pad=True):\n",
    "        super(embed,self).__init__()\n",
    "        self.lookup_table=nn.Embedding(vocab_size, embed_size)\n",
    "        self.vocab_size=vocab_size\n",
    "        self.num_units=num_units\n",
    "        self.zero_pad=zero_pad\n",
    "\n",
    "    def forward(self,text):\n",
    "        \n",
    "        ##if zero_pad==True, add zeros to first row\n",
    "        ## - 일단 없어서 구현 안함\n",
    "        return self.lookup_table(torch.tensor(text, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=embed(2,3,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]],\n",
       "\n",
       "         [[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]],\n",
       "\n",
       "         [[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]],\n",
       "\n",
       "         [[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]],\n",
       "\n",
       "         [[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]],\n",
       "\n",
       "         [[ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370],\n",
       "          [ 1.2395,  0.7210,  0.1317,  ...,  0.1812,  0.2391, -0.2370]]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg=torch.ones(5,2, 3,dtype=torch.float)\n",
    "model(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6194, -0.1937,  0.3778,  1.8502, -0.0589]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4587, -0.6070,  0.2611, -0.2758, -1.8443,  0.5966,  0.2077],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5, 7, dtype=torch.double)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3541, -0.2125],\n",
      "         [-0.3509, -0.2105]],\n",
      "\n",
      "        [[ 0.2125,  0.3541],\n",
      "         [ 0.2105,  0.3509]]], grad_fn=<ThnnBatchNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "sd=torch.tensor(np.array([[[1,2],[3,4]],[[5,6],[7,8]]]),dtype=torch.float)#(3,3,3)\n",
    "b=nn.BatchNorm1d(2)\n",
    "print(b(sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 5]\n",
      "  [3 7]]\n",
      "\n",
      " [[2 6]\n",
      "  [4 8]]]\n",
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]]\n"
     ]
    }
   ],
   "source": [
    "df=np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "df=np.transpose(df,(2,1,0))\n",
    "print(df)\n",
    "df=np.transpose(df,(2,1,0))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bn(nn.Module):\n",
    "    def __init__(self, activation_fn=None):\n",
    "        super(bn,self).__init__()      \n",
    "\n",
    "    def forward(self, input):\n",
    "        print(input)\n",
    "        n_features=input.shape[2]\n",
    "        print(input.shape)\n",
    "        input_t=torch.transpose(input,1,2)\n",
    "        batch_norm=nn.BatchNorm1d(n_features)\n",
    "        return torch.transpose(batch_norm(input_t),2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.],\n",
      "         [3., 4., 6.]],\n",
      "\n",
      "        [[5., 6., 7.],\n",
      "         [7., 8., 9.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[-0.0375, -0.5231, -0.6180],\n",
      "         [-0.0125, -0.1744, -0.0475]],\n",
      "\n",
      "        [[ 0.0125,  0.1744,  0.1426],\n",
      "         [ 0.0375,  0.5231,  0.5229]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m=bn()\n",
    "print(m(torch.tensor(np.array([[[1,2,3],[3,4,6]],[[5,6,7],[7,8,9]]]),dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(max(int(3/2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "size=5\n",
    "pad_size = max(int((size-1)/2),int(size/2))\n",
    "print(pad_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv1d(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input : (N,Tx, E/2)\n",
    "    channels : input.shape[2]\n",
    "        \n",
    "    (N,C,L) - N,channel(E/2), length(Tx)로 바꾼 후 conv계산 -> 다시 N,Tx,E/2 꼴로 바꿔줘야 함\n",
    "    pytorch에서는 in_channel이 shape[1], tensorflow에서는 in_channel이 shape[2], shape[0]이 batch 크기인 건 공통\n",
    "       \n",
    "    https://discuss.pytorch.org/t/output-shape-of-conv1d-in-pytorch-and-keras-are-different/3398 참고\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, filters=None, size=1, rate=1, padding=\"SAME\", use_bias=False, activation_fn=None):\n",
    "        super(conv1d,self).__init__()\n",
    "        self.activation_fn=activation_fn\n",
    "        if padding == \"SAME\":\n",
    "            pad_size = max(int((size-1)/2),int(size/2))\n",
    "        else:\n",
    "            pad_size=0\n",
    "        # shape[2] = E/2\n",
    "        conv=torch.nn.Conv1d(channels, filters, kernel_size=size, stride=rate, padding=pad_size, bias=use_bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #padding=causal은 아직 없는 것 같아서 일단 안만듦\n",
    "        #filter=None도 일단 생략\n",
    "        input_t=torch.transpose(input,1,2)\n",
    "        if self.activation_fn != None:\n",
    "            return torch.transpose(conv(input_t),2,1)\n",
    "        else:\n",
    "            #activation function 아직 구현 안함\n",
    "            return torch.transpose(conv(input_t),2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'in_channels' and 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-acf945acbf9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-23a605a7ab97>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, size, rate, padding, use_bias, activation_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mpad_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mconv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'in_channels' and 'out_channels'"
     ]
    }
   ],
   "source": [
    "m=conv1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(2, 3)\n",
    "print(a.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3129, -0.2677, -0.1801])\n"
     ]
    }
   ],
   "source": [
    "b=torch.randn(a.shape)\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv1d_banks(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    input : (N,Tx,E/2)\n",
    "    channels : input.shape[2]\n",
    "    output : (N,Tx,K*E/2)\n",
    "    K : size of Banks (filter size)...like N gram\n",
    "    iteratively apply conv1d and apply relu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,K):\n",
    "        super(conv1d_banks,self).__init__()\n",
    "        self.conv_list=nn.ModuleList()\n",
    "        self.conv_list.append(conv1d(hp.embed_size//2,hp.embed_size//2,size=1))\n",
    "        for i in range(2,K):\n",
    "            self.conv_list.append(conv1d(hp.embed_size//2,hp.embed_size//2,size=i))\n",
    "    def forward(self):\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv1d_banks(\n",
       "  (conv_list): ModuleList(\n",
       "    (0): conv1d()\n",
       "    (1): conv1d()\n",
       "    (2): conv1d()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_banks(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bn(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input = 3D\n",
    "    batch normalization on the (N,Tx)\n",
    "    n_features = input.shape[2] (E/2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, activation_fn=None):\n",
    "        super(bn,self).__init__() \n",
    "        self.activation_fn=activation_fn\n",
    "        batch_norm=nn.BatchNorm1d(n_features)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        #E/2  ->  batch norm should be done except last dim (E/2) (input:(N,Tx,E/2))  \n",
    "        #E/2(2) to Position of C(1)\n",
    "        input_t=torch.transpose(input,1,2) \n",
    "\n",
    "        #restore shape\n",
    "        rst = torch.transpose(batch_norm(input_t),2,1)\n",
    "\n",
    "        if self.activation_fn==\"ReLU\":\n",
    "            return torch.nn.ReLU(rst)\n",
    "        else:\n",
    "            return rst  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv1d(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input : (N,Tx, E/2)\n",
    "    channels : input.shape[2]\n",
    "        \n",
    "    (N,C,L) - N,channel(E/2), length(Tx)로 바꾼 후 conv계산 -> 다시 N,Tx,E/2 꼴로 바꿔줘야 함\n",
    "    pytorch에서는 in_channel이 shape[1], tensorflow에서는 in_channel이 shape[2], shape[0]이 batch 크기인 건 공통\n",
    "       \n",
    "    https://discuss.pytorch.org/t/output-shape-of-conv1d-in-pytorch-and-keras-are-different/3398 참고\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, filters=None, size=1, rate=1, padding=\"SAME\", use_bias=False, activation_fn=None):\n",
    "        super(conv1d,self).__init__()\n",
    "        self.activation_fn=activation_fn\n",
    "\n",
    "        #filters=None이면 filter의 수는 input channel의 수와 같게 (E/2)\n",
    "        if filters == None:\n",
    "            filters = channels\n",
    "\n",
    "        if padding == \"SAME\":\n",
    "            pad_size = max(int((size-1)/2),int(size/2))\n",
    "        else:\n",
    "            pad_size=0\n",
    "        # shape[2] = E/2\n",
    "        conv=torch.nn.Conv1d(channels, filters, kernel_size=size, stride=rate, padding=pad_size, bias=use_bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        #padding=causal은 아직 없는 것 같아서 일단 안만듦\n",
    "        #filter=None도 일단 생략\n",
    "\n",
    "        #channel size should be E/2, which is at shape[2]. So reshape it to locate at shape[1]\n",
    "        input_t=torch.transpose(input,1,2)\n",
    "        rst=torch.transpose(conv(input_t),2,1)\n",
    "        if self.activation_fn == \"ReLU\":\n",
    "            return torch.nn.ReLU(rst)\n",
    "        else:\n",
    "            return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv1d_banks(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    input : (N,Tx,E/2)\n",
    "    channels : input.shape[2]\n",
    "    output : (N,Tx,K*E/2)\n",
    "    K : size of Banks (filter size)...like N gram\n",
    "    iteratively apply conv1d and apply relu\n",
    "\n",
    "    rst : concat the conv1 results from size 1 to K\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,K):\n",
    "        super(conv1d_banks,self).__init__()\n",
    "        self.conv_list=nn.ModuleList()\n",
    "        self.K=K\n",
    "        for i in range(K):\n",
    "            self.conv_list.append(conv1d(hp.embed_size//2,hp.embed_size//2,size=i,activation_fn=\"ReLU\"))\n",
    "        self.bn = bn(hp.embed_size//2,activation_fn=\"RELU\")\n",
    "            \n",
    "    def forward(self,input):\n",
    "        rst = conv_list[0](input)\n",
    "        for i in range(2,K):\n",
    "            output=conv_list[i](input) # (N,Tx,E/2) 꼴로 나옴\n",
    "            torch.cat((rst,output),2)\n",
    "        rst=bn(rst)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gru(nn.Module):\n",
    "    \"\"\"\n",
    "    apply gru\n",
    "    inputs : (N,Time(T), Channel(E/2...))\n",
    "    __init__ 단계에서 module을 선언해줘야 해서 num_unit=None이면 안됨, shape[2]라도 넣어줘야...그런데 network에서는 그럴 일은 없는듯\n",
    "\n",
    "    num_units = # of hidden units passed\n",
    "\n",
    "    num_layers = # of rnn layers (# of cells)\n",
    "    \n",
    "    input of torch.rnn.gru : (seq_len, batch, input_size) <- (T, N, channel)\n",
    "    output of torch.rnn.gru : (seq_len, batch, num_directions(bidirectional이므로 2) * hidden_size)->(T, N, num_units)\n",
    "    --> should be transposed (bidirectional의 경우 알아서 concat되어서 나오는 듯.., 확인 필요)\n",
    "\n",
    "    output : (N,T,num_units) if bidirectional, num_units*2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, shape, num_units, bidirection=False):\n",
    "        super(gru,self).__init__()\n",
    "        if bidirection == True:\n",
    "            self.gru = torch.nn.GRU(input_size=shape[2] ,num_layers=shape[1], hidden_size=num_units, bidirection=True)\n",
    "        else:\n",
    "            self.gru = torch.nn.GRU(input_size=shape[2], num_layers=shape[1], hidden_size=num_units, bidirection=False)\n",
    "\n",
    "    \n",
    "    def forward():\n",
    "        input_t = torch.transpose(input,0,1)\n",
    "        output,hn = self.gru(input_t)   #hn : hidden states for each t\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "m=torch.nn.Linear(6,3)\n",
    "input=torch.randn(2,3,6)\n",
    "#input.view(-1,6)\n",
    "print(input.shape)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prenet(nn.Module):\n",
    "    \"\"\"\n",
    "    input : (N, T(Tx or Ty/r), E/2)\n",
    "    output : (N, T, num_units/2)\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, num_units=None):\n",
    "        super(prenet,self).__init__()\n",
    "        if num_units == None:\n",
    "            num_units=[hp.embed_size,hp.embed_size//2]\n",
    "        self.dense1 = torch.nn.Linear(shape[2],num_units[0])\n",
    "        self.dropout1 = torch.nn.Dropout(p=hp.dropout_rate)\n",
    "        self.dense2 = torch.nn.Linear(num_units[0],num_units[1])\n",
    "        self.dropout2 = torch.nn.Dropout(p=hp.dropout_rate)\n",
    "\n",
    "    def forward(self,input):\n",
    "        rst = self.dense1(input)\n",
    "        rst = self.dropout1(rst)\n",
    "        rst = self.dense2(rst)\n",
    "        rst = self.dropout2(rst)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=prenet(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3393, -0.2029,  0.2975,  0.4790,  0.1326,  0.0000,  0.0000,\n",
       "          -0.3741,  1.0512, -0.0000,  0.0000,  0.0000,  0.1010, -0.0000,\n",
       "           0.7549,  0.6607, -0.6049,  0.0000, -0.2832, -0.0000, -0.7248,\n",
       "           0.7468,  1.2562, -0.5010, -0.0000, -0.8849, -0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.6694, -0.0000,  0.0000, -0.6123, -0.9743,\n",
       "          -0.0000,  0.0000, -0.0000, -0.4713,  0.2421, -0.0000,  0.0000,\n",
       "          -0.0000,  0.0000,  0.3568,  0.7076, -0.0000, -0.0000,  0.6753,\n",
       "           0.0474,  0.0000, -0.5627, -0.0000,  0.0000,  0.0000, -0.0000,\n",
       "           2.1279,  0.3546,  0.0000,  0.0000, -0.0000, -0.8776,  0.0000,\n",
       "           0.0000,  0.0000,  0.2188,  0.3458,  0.0000,  1.0520, -0.0000,\n",
       "          -0.1902,  0.0000,  0.8887, -0.0000, -0.1387,  0.0000,  0.0196,\n",
       "           0.0000, -0.0197, -0.4207,  0.0000, -0.0000,  0.0000,  0.6754,\n",
       "          -0.0000, -1.2039, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "           0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -2.0941, -1.3966,\n",
       "          -0.1376,  0.0000,  0.0000, -0.0000, -0.0959,  0.0000,  0.0000,\n",
       "           0.2709, -0.0000,  2.0622,  0.0000, -0.4217,  0.3047, -0.2116,\n",
       "          -0.0000,  2.0500,  0.4291,  0.0000,  0.0000,  0.2292,  0.8746,\n",
       "          -0.0000,  0.0000,  0.2967, -0.0000, -0.3250, -0.0000, -0.0000,\n",
       "          -0.0000,  0.5010],\n",
       "         [-0.0000,  0.2810,  0.0295, -1.1694,  1.6373, -0.0000,  2.1555,\n",
       "           0.0392,  0.0000,  0.8521,  1.3105, -0.0918,  0.0000,  0.5311,\n",
       "           1.5861, -0.3627, -0.0000, -0.0000,  0.0000, -2.1626, -0.5094,\n",
       "          -0.0000,  1.0179, -0.7647, -0.0000, -0.0000, -0.0000,  0.9605,\n",
       "           0.0000, -1.0483,  1.0886,  0.0000, -0.0000, -0.0000,  0.5891,\n",
       "           1.0304,  1.5398,  0.7161, -0.0000,  0.1580,  0.1305,  0.0000,\n",
       "           0.0000,  0.0000,  1.3432, -0.0000, -1.2805, -1.1834,  0.0000,\n",
       "           0.4405,  0.0000,  0.0000,  1.4819, -1.4814, -0.0000, -0.0000,\n",
       "           0.0000, -0.0000, -1.2929, -0.3519, -0.0699,  0.6858, -0.3287,\n",
       "           0.0000,  0.0000, -0.0000,  1.1570,  0.0000, -0.1799, -0.0000,\n",
       "          -1.9074,  0.0000,  0.0000, -0.7837, -0.0796,  0.8945,  0.0000,\n",
       "          -1.3806, -0.0774, -0.0000,  1.0387, -0.0000,  0.0000, -0.0000,\n",
       "           0.5938, -2.4451, -1.8646, -0.0000, -0.0000,  0.2062,  1.5431,\n",
       "           1.1012, -0.3630, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "           0.0000,  1.0615,  1.8049, -1.3797, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000, -1.6889,  0.4640, -0.1224,\n",
       "           0.0000, -0.8755, -0.4189,  0.0000, -1.5839, -0.0000,  0.0000,\n",
       "          -0.8119, -0.0000, -1.0140, -1.8796, -0.0000,  0.3381, -1.0527,\n",
       "          -0.8053, -1.3108],\n",
       "         [-0.0000, -0.0000, -1.2856, -3.3085,  0.4541, -0.0000,  1.2784,\n",
       "          -1.4747,  0.1328,  0.9570, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "           1.3785,  0.6470, -0.0000, -0.0000,  0.9300, -0.0000,  0.0000,\n",
       "          -0.6782, -0.1640,  1.4144,  1.9143,  2.3021, -0.0000,  0.0000,\n",
       "          -0.4544, -0.0000,  0.5295, -0.2232,  0.0546, -2.2710,  0.9126,\n",
       "           0.6908,  3.0034,  0.1015,  0.4133,  0.0000, -0.0000,  0.0000,\n",
       "           0.0000, -0.7349, -0.0000,  0.0000, -3.0055, -2.3170, -1.0027,\n",
       "          -0.0000, -0.2753,  0.0000,  1.1771,  0.5934, -1.1249, -0.0000,\n",
       "          -0.0000,  1.6395,  0.8764, -0.6232,  0.0000, -0.1357,  0.0971,\n",
       "           0.0000,  0.0000,  0.0000,  3.9071,  0.0000, -0.5463,  0.1173,\n",
       "          -0.0000, -2.6378,  0.0000, -0.0000,  0.4547,  0.0000,  0.0000,\n",
       "          -0.1110,  0.0000, -1.2960, -1.7446, -0.0000, -0.0000, -0.0000,\n",
       "           2.2502, -0.0000,  0.0000,  0.0000, -0.0000, -1.7327,  0.0000,\n",
       "          -0.7934,  2.7953, -0.0000,  0.0000, -1.6037, -0.0000,  0.0000,\n",
       "           2.0053,  1.3618, -0.0000, -0.5596, -1.9716, -0.0000,  2.0168,\n",
       "           0.0000,  1.0136,  0.0000,  0.0000, -0.0000,  0.0000, -0.5576,\n",
       "           0.4767, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000,  1.6229,  0.0000,  0.3792, -0.0000,\n",
       "           0.8363,  0.7098]],\n",
       "\n",
       "        [[ 0.0000, -0.2848,  0.0000,  0.3675, -0.0000, -0.5619,  0.0000,\n",
       "          -0.1556,  0.0000, -0.0000,  0.2073, -0.0713, -0.2809,  1.7809,\n",
       "           0.0000, -1.2274, -0.6284, -0.0000, -0.0000,  0.6040, -0.8606,\n",
       "           0.0000,  0.0000, -0.9740,  0.2660,  0.0000, -0.4550,  0.0000,\n",
       "           0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -1.0051,  0.0148,\n",
       "           0.0281,  0.9321,  0.7220,  0.2621, -0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  1.1848,  0.1560,  0.2388, -0.0000, -0.0000, -0.0000,\n",
       "          -0.0000,  0.1973, -0.0439,  0.0000,  0.1536,  0.0000,  0.5981,\n",
       "          -0.0000, -0.3198,  0.2128, -0.0579,  0.2255, -0.0000,  0.0000,\n",
       "          -0.0000,  0.0213,  0.0000, -0.6232, -0.0000,  0.0000,  1.5941,\n",
       "          -0.8626, -0.3049, -0.0000, -0.0000, -0.4067,  1.0595,  0.5844,\n",
       "          -0.5050,  0.0000, -0.0000,  0.1536, -0.0000, -1.0394,  0.0000,\n",
       "           0.0000, -0.8020, -0.0000, -0.4532, -0.0000, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000,  0.1773, -0.9924,  0.7373, -0.0000,  0.1477,\n",
       "          -0.0000,  0.2313, -0.8247, -0.0004, -0.0000,  0.2808,  0.0000,\n",
       "          -0.0000,  0.7375,  1.6405,  0.0672, -0.0431,  0.8182, -0.1510,\n",
       "           0.0000, -0.0002, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.5255, -0.3939, -0.0000, -1.0694, -0.0000,\n",
       "           0.4015,  1.8282],\n",
       "         [ 1.7170, -1.9773,  0.0000,  0.2486,  0.0000,  0.2883, -0.9472,\n",
       "           1.0527,  2.9443, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -0.0000, -0.6299, -0.0000, -0.0000,  1.7778, -0.0000, -0.0000,\n",
       "           0.0000, -1.6470,  1.4615,  0.0000,  0.0000, -0.0000,  2.9375,\n",
       "          -0.7471, -0.0000, -0.0000, -0.2280,  0.0000, -0.0000, -0.0000,\n",
       "           0.0000, -1.2491, -0.8501,  1.5223, -0.0000, -1.9738,  2.3119,\n",
       "          -0.7905, -0.0000,  0.1318, -1.0341, -0.0000, -0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  1.7317,  0.0000, -0.0000, -0.0000, -0.5560,\n",
       "          -2.9908, -1.5358,  0.0000,  0.0000, -0.0000, -0.0000,  0.5361,\n",
       "           2.6601, -0.0000, -0.0000, -0.0000,  0.6341, -0.0000,  0.0000,\n",
       "           1.1289,  0.9836, -0.0000, -0.5627, -0.7149,  0.0000,  2.3152,\n",
       "           2.1109,  0.3554,  2.5281, -1.6315,  1.0730,  0.0000,  0.0000,\n",
       "          -0.0000, -0.0000,  1.9918,  1.3007, -1.3999, -1.5003, -0.1961,\n",
       "           0.0000,  3.6721,  2.1979, -0.0000,  0.0000, -1.7556,  0.0000,\n",
       "          -0.0815,  1.0609,  0.0000, -2.3706, -1.5306,  1.1414, -0.0000,\n",
       "           0.0000, -0.3909,  1.3731, -0.3470, -0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0725,  0.1764, -0.2093,  2.1479,  1.1284,  3.7709,\n",
       "           1.2910, -0.0051,  0.0000, -2.5665, -0.4250, -0.0000, -0.0000,\n",
       "          -2.5238,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.6160,  0.8046, -0.1199,  0.0000,  1.0238,\n",
       "           0.0000,  1.1754, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "          -0.0943,  1.3631, -0.1471, -0.0000,  0.3680, -0.0000, -0.0000,\n",
       "          -0.0000,  0.0000,  0.2752,  0.3827, -0.0000, -0.0000,  0.0000,\n",
       "          -0.0000, -0.8955, -0.0000,  1.6144, -0.5609, -1.4664, -0.8145,\n",
       "           0.4951,  0.0000, -0.0000,  0.9547, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0000, -1.0273,  0.0000, -0.0000, -1.2403, -1.3515, -0.0000,\n",
       "          -0.0000, -1.8810,  1.3051,  1.3597,  0.1032,  0.0000, -0.0000,\n",
       "           0.2122,  0.0426,  0.0000,  0.0000,  0.0000,  0.0000,  0.0929,\n",
       "           1.4437,  0.3778, -0.0041, -1.7878,  0.0000, -0.0000,  0.4151,\n",
       "           0.0000, -0.0000,  0.2063,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "           0.0000, -0.0000,  2.6194, -0.6068,  0.0000,  0.0000,  0.0324,\n",
       "           0.3204, -1.8794,  0.1376, -0.0000, -0.0000, -1.8916,  0.0000,\n",
       "          -2.7395,  0.8529,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "           0.0000,  2.0928,  1.6297, -0.0000,  0.4972, -0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000, -0.1162,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.6858,  0.0000,\n",
       "          -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0243, -0.0000,\n",
       "           0.0000,  0.1710]]], grad_fn=<DropoutBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7977,  1.3582, -0.4157,  0.6353, -0.1259,  1.5807],\n",
       "         [ 0.0642,  3.0698,  0.1724,  0.8985, -0.0285,  0.9297],\n",
       "         [ 0.8839,  1.3785, -0.0805, -1.9381,  1.0947, -0.4266]],\n",
       "\n",
       "        [[ 1.7508,  0.5107, -0.6384,  0.4805,  1.5560,  1.3023],\n",
       "         [ 0.1246, -0.5249,  0.2613,  2.7864, -0.2311, -0.3898],\n",
       "         [ 1.5718, -0.1571, -0.2735,  0.5759, -1.1591,  1.0509]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class highwaynet(nn.Module):\n",
    "    \"\"\"\n",
    "    inputs : (N, T, W) - (ex> N, Tx or Ty/r, E/2)\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, num_units=None):\n",
    "        super(highwaynet,self).__init__()\n",
    "        if num_units == None:\n",
    "            num_units = shape[2]\n",
    "        self.H = torch.nn.Linear(shape[2],num_units)\n",
    "        self.T = torch.nn.Linear(shape[2],num_units)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        h_relu = self.relu(self.H(inputs))\n",
    "        t_sigmoid = self.sigmoid(self.T(inputs))\n",
    "        output = h_relu * t_sigmoid + inputs * (1.-t_sigmoid)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=highwaynet(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 3)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b18c85dc1210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAttn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b18c85dc1210>\u001b[0m in \u001b[0;36mAttn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAttn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = GeneralAttn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0298e5f9685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mBahdanauAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-92dcbebbbc65>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_size, output_size, n_layers, dropout_p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBahdanauAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Define parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "m= BahdanauAttnDecoderRNN(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,3,9, dtype=torch.double)\n",
    "print(a.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=a.shape\n",
    "k[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=(a.shape[0],a.shape[1],a.shape[2]/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class decoder1(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "        inputs : (N, Ty/r, n_mels)\n",
    "        memory : (N, Tx, E)\n",
    "\n",
    "        return : (N, Ty/r, n_mels*r)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, shape, is_training): #여기서 shape는 Go의 shape여야 함\n",
    "        super(decoder1, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        self.shape = shape\n",
    "        self.prenet = prenet(self.shape)\n",
    "        self.attention_rnn = AttentionWrapper(\n",
    "            nn.GRUCell(256 + 128, 256),\n",
    "            BahdanauAttention(256)\n",
    "        )\n",
    "        self.decoder_rnns = nn.ModuleList(\n",
    "            [nn.GRUCell(256, 256) for _ in range(2)])\n",
    "        self.memory_layer = nn.Linear(256, 256, bias=False)\n",
    "        self.proj_to_mel = nn.Linear(256, hp.n_mels*hp.r)\n",
    "\n",
    "    def forward(self, inputs, memory):\n",
    "        N = memory.size(0)\n",
    "        processed_memory = self.memory_layer(memory)\n",
    "        \"\"\"\n",
    "        if memory_lengths is not None:\n",
    "            mask = get_mask_from_lengths(processed_memory, memory_lengths)\n",
    "        else:\n",
    "            mask = None\n",
    "        \"\"\"\n",
    "        # Run greedy decoding if inputs is None (at Training time)\n",
    "        greedy = (self.is_training==True)\n",
    "\n",
    "        if self.is_training==False:\n",
    "            # Grouping multiple frames if necessary\n",
    "            if inputs.size(-1) == hp.n_mels:\n",
    "                inputs = inputs.view(N, inputs.size(1) // hp.r, -1)\n",
    "            assert inputs.size(-1) == hp.n_mels * hp.r\n",
    "            T_decoder = inputs.size(1)\n",
    "\n",
    "        # go frames\n",
    "        initial_input = Variable(\n",
    "            memory.data.new(N, hp.n_mels * hp.r).zero_())\n",
    "\n",
    "        while True:\n",
    "            if t > 0:\n",
    "                current_input = outputs[-1] if greedy else inputs[t - 1]\n",
    "            # Prenet\n",
    "            current_input = self.prenet(current_input)\n",
    "\n",
    "            # Attention RNN\n",
    "            attention_rnn_hidden, current_attention, alignment = self.attention_rnn(\n",
    "                current_input, current_attention, attention_rnn_hidden,\n",
    "                encoder_outputs, processed_memory=processed_memory, mask=mask)\n",
    "\n",
    "            # Concat RNN output and attention context vector\n",
    "            decoder_input = self.project_to_decoder_in(\n",
    "                torch.cat((attention_rnn_hidden, current_attention), -1))\n",
    "\n",
    "            # Pass through the decoder RNNs\n",
    "            for idx in range(len(self.decoder_rnns)):\n",
    "                decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n",
    "                    decoder_input, decoder_rnn_hiddens[idx])\n",
    "                # Residual connectinon\n",
    "                decoder_input = decoder_rnn_hiddens[idx] + decoder_input\n",
    "\n",
    "            output = decoder_input\n",
    "            output = self.proj_to_mel(output)\n",
    "\n",
    "            outputs += [output]\n",
    "            alignments += [alignment]\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            if t >= T_decoder:\n",
    "                break\n",
    "        assert greedy or len(outputs) == T_decoder\n",
    "\n",
    "        alignments = torch.stack(alignments).transpose(0, 1)\n",
    "        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n",
    "\n",
    "        return outputs, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
